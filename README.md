# awesome-sparsely-activated-dl-models
This repo include sparsely activated DL papers, the list is not comprehensive yet.

## Surveys 
- Han, Y., Huang, G., Song, S., Yang, L., Wang, H., & Wang, Y. (2021). Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7436-7456.

## Computer Vision
### ConvNet-based Architectures:
- Yu, J., Yang, L., Xu, N., Yang, J., & Huang, T. (2018). Slimmable neural networks. arXiv preprint arXiv:1812.08928.
- Neseem, M., & Reda, S. (2021, November). AdaCon: Adaptive Context-Aware Object Detection for Resource-Constrained Embedded Devices. In 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD) (pp. 1-9). IEEE.


### Transformer-based Arcitectures:
- Tang, Y., Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., & Tao, D. (2022). Patch slimming for efficient vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12165-12174).
- Yu, Z., Fu, Y., Li, S., Li, C., & Lin, Y. (2022, June). Mia-former: efficient and robust vision transformers via multi-grained input-adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 8, pp. 8962-8970).
- Meng, L., Li, H., Chen, B. C., Lan, S., Wu, Z., Jiang, Y. G., & Lim, S. N. (2022). Adavit: Adaptive vision transformers for efficient image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12309-12318).
- Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., ... & Houlsby, N. (2021). Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34, 8583-8595.
- Neseem, Marina, Ahmed Agiza, and Sherief Reda. "AdaMTL: Adaptive Input-dependent Inference for Efficient Multi-Task Learning." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

## Language Models
### LSTM-based Arcitectures:
- Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.

### Transformer-based Arcitectures:
- Xin, J., Tang, R., Lee, J., Yu, Y., & Lin, J. (2020). DeeBERT: Dynamic early exiting for accelerating BERT inference. arXiv preprint arXiv:2004.12993.
- Zhou, W., Xu, C., Ge, T., McAuley, J., Xu, K., & Wei, F. (2020). Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33, 18330-18341.
- Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1), 5232-5270.
- Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., & Chen, Z. (2020). Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.
- Correia, G. M., Niculae, V., & Martins, A. F. (2019). Adaptively sparse transformers. arXiv preprint arXiv:1909.00015.

## Other Applications:
- M. Neseem, J. Nelson and S. Reda, "AdaSense: Adaptive Low-Power Sensing and Activity Recognition for Wearable Devices," 2020 57th ACM/IEEE Design Automation Conference (DAC), San Francisco, CA, USA, 2020, pp. 1-6, doi: 10.1109/DAC18072.2020.9218568.
- A. N.K., G. Bhat, J. Park, H. G. Lee and U. Y. Ogras, "Sensor-Classifier Co-Optimization for Wearable Human Activity Recognition Applications," 2019 IEEE International Conference on Embedded Software and Systems (ICESS), Las Vegas, NV, USA, 2019, pp. 1-4, doi: 10.1109/ICESS.2019.8782506.
